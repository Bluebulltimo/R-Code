#This example uses a number of different models to assess the data, looking for the best fit.  
#The example begins with the data already split.

library(caret)
library(C50)
library(gbm)
library(plyr)
library(pROC)
library(kernlab)
library(earth)
library(mda)
data(churn)
str(churnTrain)
predictors <- names(churnTrain)[names(churnTrain) != 'churn']

#How to Recombine the Split Data for Custom Data Partition 
#allData <- rbind(churnTrain, churnTest)
#set.seed(1)
#inTrainingSet <- createDataPartition(allData$Churn, p = 0.75, list = FALSE)
#churnTrain <- allData[inTrainingSet, ]
#churnTest <- allData[-inTrainingSet, ]

#Following Code Works with Original Partition

#Preprocessing the Data to Find Mean and Standard Deviation
numerics <- c('account_length', 'total_day_calls', 'total_night_calls')
procValues <- preProcess(churnTrain[, numerics], 
                         method = c('center', 'scale', 'YeoJohnson'))
trainScaled <- predict(procValues, churnTrain[, numerics])
testScaled <- predict(procValues, churnTest[, numerics])
procValues

#Boosting the Model with the GBM Function (This Model is not Tuned)
#forGBM <- churnTrain
#forGBM$churn <- ifelse(forGBM$churn == 'yes', 1, 0)
#gbmFit <- gbm(formula = churn~. , distribution = 'bernoulli', 
              #data = forGBM, n.trees = 2000, interaction.depth = 7, 
              #shrinkage = 0.01, verbose = FALSE)

#Tuning a GBM Model (Note: Run-time is very long)
ctrl <- trainControl(method = 'repeatedcv', 
                     repeats = 5, 
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)
grid <- expand.grid(n.trees = seq(100, 1000, by = 50), 
                    interaction.depth = seq(1, 7, by = 2), 
                    shrinkage = c(0.01, 0.1), 
                    n.minobsinnode = 0)
set.seed(1)
gbmTune <- train(churn~. , data = churnTrain, 
                 method = 'gbm', 
                 metric = 'ROC', 
                 verbose = FALSE, 
                 trControl = ctrl, 
                 tuneGrid = grid)

#Viewing the Results
ggplot(gbmTune) + theme(legend.position = 'top')

#Using the Tuned Model for Prediction
gbmPred <- predict(gbmTune, churnTest)
str(gbmPred)
gbmProbs <- predict(gbmTune, churnTest, type = 'prob')
str(gbmProbs)

#Create a Confusion Matrix
confusionMatrix(gbmPred, churnTest$churn)

#Examine the Test Set's ROC Curve
library(pROC)
rocCurve <- roc(response = churnTest$churn, 
                predictor = gbmProbs[, 'yes'], 
                levels = rev(levels(churnTest$churn)))
rocCurve
plot(rocCurve)

#Now, Try Different Models on the Data

#Start with a Support Vector Machine
set.seed(1)
svmTune <- train(churn~. , data = churnTrain, 
                 method = 'svmRadial', 
                 preProc = c('center', 'scale'), 
                 tuneLength = 10, 
                 trControl = ctrl, 
                 metric = 'ROC')
ggplot(svmTune) + theme(legend.position = 'top')

#End with a Flexible Discriminant Model
set.seed(1)
fdaTune <- train(churn~. , data = churnTrain, 
                 method = 'fda', 
                 tuneLength = 10, 
                 trControl = ctrl, 
                 metric = 'ROC')
ggplot(fdaTune) + theme(legend.position = 'top')


#Code from the Presentation:
#The Caret Package: A United Interface for Predictive Models
#Kuhn, Max
#February 26, 2014
